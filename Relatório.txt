Dia 1:
Statlog Dataset:

Acurácia da Decision Tree: 
- 69,11% para dataset pre-processado 
- 67.64% com dataset original.

obs: tamanho do conjunto de testes: 25%

A Feature "Age" teve P-value de 0.799, as demais obtiveram P-value inferior a 0.15 
de acordo com api statsmodels.formula do python.

Dia 2:

Na plataforma Gretl a feature "Age" obteve o P-value superior a 0.50 e o modelo com
a variável apresentou a métrica adjusted R-squared de 0.522155, ao retirar esta feature
o modelo apresentou a métrica adjusted R-squared de 0,523199, outras features obtiveram
o p-value acima de 0.05, porém ao retira-las percebe-se uma queda na métrica adjusted
R-squared, portanto vejo como válido mantê-las.

Acurácia da Decision Tree:
-69.11% para o dataset original sem a feature "Age"
-75% para o dataset pre-processado sem a feature "Age"

obs: tamanho do conjunto de testes: 25% da base de dados

Acurácia da Decision Tree:
-75.92% para o dataset pre-processado sem a feature "Age"
tamanho do conjunto de testes: 20%.

-85.18% para o dataset pre-processado sem a feature "Age"
tamanho do conjunto de testes: 10%

Cleveland Dataset:

No presente Dataset a feature idade apresenta o p-value de 0.193, outra diferença em 
relação ao Dataset de Statlog é que a feature "chol" possui o p-value de 0.543, o que o
torna passível de extração. 

Acurácia da Decision Tree:
-70.49% para o dataset original
-75.40% para o dataset pre-processado

Acurácia da Decision Tree:
-67.21% para o dataset original sem a feature "age"
-73.77% para o dataset pre-processado sem a feature "age" 

Statlog e Cleveland Datasets:

Acurácia da Decision Tree:
-99.25% usando o dataset pre-processado de cleveland para treino e de statlog para teste
-96.69% usando o dataset pre-processado de statlog para treino e de clevelando para teste

Dia 3:

Foi observado que a base de dados de cleveland estava pre-processada os valores que
na base de dados original estavam ausentes foram alterados para "-1". Visto isso
foi utilizado a biblioteca "Imputer" para cuidar dos dados ausentes, foi usado a
estratégia do mais frequente(Moda) para substituir os dados ausentes. Assim novos
testes foram realizados usando a base de dados original de cleveland. Os mesmos
resultados do dia anterior foram obtidos (99.25% de acurácia usando a base pre-processada).

Dia 4:

Observando que os testes anteriores no datasets separados "DT_cleveland_prediction.py" e 
"DT_statlog_prediction.py" foram feitos usando método hold-out para treino e predição
dos casos, estes novos testes foram feitos com o método 10-fold cross validation para análise
dos resultados usando a acurácia como métrica.

obs: as bases de dados usadas foram as completas com as 13 features aconselhadas pela
especificação das bases, os dados foram padronizados para ficarem na mesma escala e os
dados ausentes foram preenchidos com a moda no caso da base de dados de cleveland.

Resultados: 
	DT_cleveland_prediction.py: 75.49%
	DT_statlog_prediction.py: 72.59%

Dia 5:

Análises utilizando O Software Tableau 10.4 foram feitas na base de dados de cleveland.
Apenas uma parte da bateria de análises foram feitas, basicamente apenas para visualização
de variáveis categóricas e usando agrupamentos nas variáveis discretas. Em todos os casos
é usado a feature "Diagnostic" para visualização da quantidade de Pessoas com doença do
coração ou não.

Obs: A base de dados foi pre-processada, a variável dependente teve teve os valores
originais modificados, inicialmente eram [0-4] sendo que '0' representava chances menores
de 50% de o paciente possuir doença do coração e a partir de '1' as representava chances
maiores de 50%, aumentando a gravidade. O pre-processamento foi feito visando simplificar
o diagnóstico mudando os valores diferentes de '0' e '1' para '1', assim representando
apenas chances maiores de 50% de se ter doença do coração.

Para gerar novos gráficos foi considerado usar o p-value das features para mostrar de forma
visual o quanto elas afetam no diagnóstico.

Obs: O p-value foi encontrado usando o Mínimo Quadrados Ordinários (Ordinary Least Squares)
do software Gretl.

Features	 P-value      
Thal		 1,04e^-05    
Age		 0,8024
Sex		 0,0021   
CP		 0,0002   
Trestbps	 0,1160
Chol		 0,3322
Fbs		 0,2497
Restecg		 0,1216
Thalach		 0,0344
Exang		 0,0030
Oldpeak		 0,1026	      
Slope		 0,1399	      
CA		 2,18e^-07

R-quadrado ajustado: 0,505516

O Gráfico que apresenta a feature Ca por Thal mostra que com Thal tendo o valor "Normal"
e Ca sendo igual a '0', há 117 casos representando mais de 33% dos casos da base de dados
e desses, 103 apresentam chances menores de 50% de possuir doença do coração.
 
Para Thal tendo o valor "Fixed Defect", o número de total de
casos é 19, não apresentando nem 10% da base de dados e desses apenas 7 possuem o risco de
ter doença do coração menor do que 50%, ou seja diagnostic tendo o valor '0'. É visível
também a predominancia de pacientes com chances maiores de 50% de ter doença do coração no
caso de Thal ter o valor "Reversable Defect" o qual possui 117 casos sendo que desses apenas
28 casos apresentam chances menores de 50% de o paciente ter doença do coração.

O Gráfico da feature "Age" mostra que há sim um crescimento no número de doenças
de coração nos pacientes com idade maior ou igual a 56 anos, portanto a feature, apesar de
ter apresentado p-value de 0,8024 no teste OLS do Gretl, deve ser considerado para o diagnóstico
de doença do coração.

O gráfico da feature Chol mostra que os valores acima de 253 apresentam maior chance se ter
doença do coração, visto que a partir deste valor a quantidade de pacientes com chances maiores
de 50% de se ter doença do coração estão acima da media geral.

No gráfico da feature Fbs, ha 258 registros com o valor "False" e 55 com o valor "True", e a
o numero de pacientes com chances maiores de 50% de ter doença do coração está na media em 
ambos os casos, o que deixa uma dúvida quanto a necessidade desta feature.

No Gretl, ao retirar a Feature Fbs por causa dos resultados observados nos gráficos é visto
que o valor da métrica R-quadrado ajustado diminui para 0,5049 o que implica que o modelo teve
perda de desempenho, assim considera-se a inserção de Fbs no modelo novamente.

Usando agora a acurácia do modelo como métrica para validação foi descoberto que com todas as
features o modelo prevê com 74,78% enquanto sem a feature "Fbs" o modelo prevê com 75.43%,
o que novamente põe a necessidade desta feature em questão.
Obs: O teste foi feito no arquivo "DT_cleveland_prediction.py" com dados pre-processados, 
usando as bibliotecas LabelEncoder para codificação dos valores categoricos, Imputer para
inserção de valores ausentes, OneHotEncoder para transformar os valores categoricos codificados
pelo LabelEncoder em Dummy Variables e a StandardScale para padronização dos valores das variáveis
numéricas contínuas.

Variáveis que faltam análises:

Restecg		 
Thalach		 
Exang		 
Oldpeak		 	      
Slope		 	 
Sex		    
CP		  
Trestbps	

Dia 6:

Com os resultados do dia anterior foi considerado retirar a feature Fbs do script
"DT_cleveland_statlog.py" obtendo sem esta uma acurácia entre 99.62% e 100%, um resultado
nao muito diferente dos resultados com a feature Fbs, o que mostra que a presença desta
não resulta em perda ou ganho no modelo de Aprendizado de Máquina.

Foi criado um template em python para treinamento e predição dos modelos 
de Aprendizado de Máquina, template este que ja foi utilizado no "DT_cleveland_prediction.py",
o código em questão ficou mais limpo e legível, com necessidade de importar apenas a biblioteca
'templates.py' criada para auxiliar os códigos em python deste projeto.

Dia 7:

O Template criado no dia anterior ("templates.py") foi usado na atualização do script 
"DT_statlog_prediction.py", funcionou bem sem alterações.

Dia 8

Foi criado um arquivo de valores separados por vírgulas com os dados de cleveland unidos
com os de statlog com o objetivo de criar uma função ou adaptar a existente para que aceite
mais de um dataset e os una de forma que seja possivel usar todos ao mesmo tempo, este 
objetivo ainda nao foi atingido, mas alguns sucessos foram obtidos, como a criação de tal
arquivo e o uso da função "classification" do "templates.py" para criação da matriz de 
confusão, validação cruzada e obtenção do classificador treinado com os dados do csv.

Acurácia da Decision Tree obtida com validação cruzado de 10 pastas (10-fold cross validation):
	97.90% com dataset 'cleveland_statlog_data.csv' pre-processado com todas as features

Matriz de confusão:
	62 00
	04 49

Mais testes estatísticos devem ser feitos para considerar a validade dos resultados.

Algumas idéias para próximos passos:

	1 Código de uma representação gráfica mais bela da Matriz de Confusão
	2 Verificar função 'change', (em que arquivo deve estar?)
	3 Aplicar outros modelos, já que apenas Decision Tree foi usada até agora
	4 Uso de GridSearch para busca dos melhores hiper-parâmetros
	5 Aplicar Rede Neural

Obs: As idéais não estão em ordem e a maioria pode ser feita de forma independente das outras

Dia 9

Função 'classification' da biblioteca 'templates.py' foi reformulada para que aceitasse
o dataframe em vez do caminho para o csv, assim o parâmetro 'names' antes utilizado foi
retirado. Seu escopo também foi alterado com o objetivo de deixar mais enxuto, com isso
novas funções foram criadas ex: 'feature_Scaling', 'train_test_classify', 'missingData'
e 'feature_target_split'.

Algumas funções específicas do problema foram retiradas a fim de deixar a biblioteca mais 
genérica, para isso foi criada a biblioteca 'utils.py'.

A idéia 2 do dia anterior (Dia 8) foi realizada.

Dia 10

Os arquivos foram organizados em pastas separadas denominadas "Scripts" para os códigos
python, "Datasets" para os conjuntos de dados e "Tableau Analysis" para as análises
realizadas no Tableau Desktop.

A idéia 1 do Dia 8 foi realizada.

a função 'plot_confusion_matrix' foi adicionada ao 'templates.py', os parâmetros necessários
são a matriz de confusão e os nomes das classes, há parâmetros adicionais como 'normalized'
que normaliza os dados da matriz, 'save' que salva a matriz apresentada com o nome 'name'
que o usuário passar.

Dia 11

Algumas ideias do Dia 8 foram reconsideradas e dispensadas no futuro próximo.

A seguir serão realizadas as idéias restantes do Dia 8:

	3 Aplicar outros modelos, já que apenas Decision Tree foi usada até agora
	4 Uso de GridSearch para busca dos melhores hiper-parâmetros
	5 Aplicar Rede Neural

Foi aplicado o modelo de Aprendizado de Máquina KNN, em comparação com A Árvore de
Decisão, o KNN obteve os melhores melhores resultados nas predições usando apenas
uma base de dados, ou seja, usando apenas o de clevelando ou apenas o de statlog,
ao utilizar as duas bases juntas, o KNN obteve resultados inferiores ao da Árvore
de Decisão, com esta acertando todos no caso de usar o de cleveland para treino e
de statlog para predição, esses resultados podem mostrar que a Árvore de Decisão
sofreu sobre-ajuste aos dados, a acurácia do KNN se manteve em média em 82%, com
variação de 2% para cima e para baixo, nos testes.

Foi gerada a figura das Matrizes de Confusão de todos os scripts usando tanto Árvore
de Decisão como KNN (com exceção do script usando Árvore de Decisão e não utilizando
o atributo 'Fbs').

O próximo modelo a ser usado será o GaussianNB do Naive Bayes